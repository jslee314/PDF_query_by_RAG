import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain.callbacks.tracers import LangChainTracer
from translator import build_translator_chain

from dotenv import load_dotenv
import os
import yaml

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2) LangChainTracer ìƒì„± (í™˜ê²½ ë³€ìˆ˜ì—ì„œ PROJECT_NAME, RUN_NAME ì½ìŒ)
tracer = LangChainTracer()

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(".cache/files", exist_ok=True)
os.makedirs(".cache/embeddings", exist_ok=True)

st.title("PDF ê¸°ë°˜ QAğŸ’¬")

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "chain" not in st.session_state:
    st.session_state["chain"] = None

# ì‚¬ì´ë“œë°” UI
with st.sidebar:
    clear_btn      = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    uploaded_files = st.file_uploader(
        "PDF íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf"],
        accept_multiple_files=True
    )
    selected_model = st.selectbox(
        "LLM ì„ íƒ",
        ["gpt-4o", "gpt-4-turbo", "gpt-4o-mini"],
        index=0
    )
    temperature = st.slider(
        "ì˜¨ë„ ì„¤ì • (temperature)",
        min_value=0.0, max_value=1.0,
        value=0.0, step=0.01
    )
    # ë²ˆì—­ ì²´ì¸ ìƒì„±
    translator_chain = build_translator_chain(
        model_name=selected_model,
        temperature=temperature
    )

def print_messages():
    for msg in st.session_state["messages"]:
        st.chat_message(msg.role).write(msg.content)

def add_message(role, content):
    st.session_state["messages"].append(ChatMessage(role=role, content=content))

@st.cache_resource(show_spinner="ë¬¸ì„œ ë¡œë“œ ë° ë¶„í•  ì¤‘ì…ë‹ˆë‹¤...")
def load_and_split(files):
    all_docs = []
    for file in files:
        path = f".cache/files/{file.name}"
        with open(path, "wb") as f:
            f.write(file.read())
        docs = PDFPlumberLoader(path).load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
        all_docs.extend(splitter.split_documents(docs))
    return all_docs

@st.cache_resource(show_spinner="ì„ë² ë”© ê°ì²´ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
def get_embeddings():
    return OpenAIEmbeddings()

@st.cache_resource(show_spinner="ë²¡í„°ìŠ¤í† ì–´(FAISS) ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
def create_vectorstore(_documents, _embeddings):
    return FAISS.from_documents(_documents, _embeddings)

def create_chain(retriever, model_name="gpt-4o", temperature=0.0):
    # YAMLì—ì„œ í”„ë¡¬í”„íŠ¸ ìŠ¤í™ ë¡œë“œ
    with open("prompts/pdf-rag.yaml", encoding="utf-8") as f:
        spec = yaml.safe_load(f)
    prompt = PromptTemplate(
        template=spec["template"],
        input_variables=spec["input_variables"],
    )
    # callback_managerë¡œ tracer ì§€ì • â†’ ëª¨ë“  LLM í˜¸ì¶œì´ LangSmithì— ê¸°ë¡ë©ë‹ˆë‹¤
    llm = ChatOpenAI(
        model_name=model_name,
        temperature=temperature,
        callbacks=[tracer]
        )
    return (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

# ë©”ì¸ ë¡œì§
if uploaded_files:
    # 1) ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
    split_docs = load_and_split(uploaded_files)
    # 2) ì„ë² ë”© ê°ì²´ ìƒì„±
    embeddings = get_embeddings()
    # 3) ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = create_vectorstore(split_docs, embeddings)
    retriever = vectorstore.as_retriever()
    # 4) ë²ˆì—­ ì²´ì¸ + ê¸°ì¡´ RAG ì²´ì¸ í•©ì„±
    rag_runnable = create_chain(
        retriever,
        model_name=selected_model,
        temperature=temperature
    )
    # 5) ì²´ì¸ ì´ˆê¸°í™”
    st.session_state["chain"] = create_chain(
        retriever,
        model_name=selected_model,
        temperature=temperature
    )

if clear_btn:
    st.session_state["messages"].clear()

print_messages()

user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
warning = st.empty()

if user_input:
    chain = st.session_state["chain"]
    if chain is not None:
        st.chat_message("user").write(user_input)
        response = chain.stream(user_input)
        with st.chat_message("assistant"):
            container = st.empty()
            answer = ""
            for token in response:
                answer += token
                container.markdown(answer)
        add_message("user", user_input)
        add_message("assistant", answer)
    else:
        warning.error("PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")
