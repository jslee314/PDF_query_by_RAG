import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate    
from dotenv import load_dotenv
import os
import yaml

# API KEY ì •ë³´ ë¡œë“œ
load_dotenv()

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(".cache/files", exist_ok=True)
os.makedirs(".cache/embeddings", exist_ok=True)

st.title("PDF ê¸°ë°˜ QAğŸ’¬")

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "chain" not in st.session_state:
    st.session_state["chain"] = None

# ì‚¬ì´ë“œë°”
with st.sidebar:
    clear_btn    = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])
    selected_model = st.selectbox("LLM ì„ íƒ", ["gpt-4o", "gpt-4-turbo", "gpt-4o-mini"], index=0)

def print_messages():
    for msg in st.session_state["messages"]:
        st.chat_message(msg.role).write(msg.content)

def add_message(role, content):
    st.session_state["messages"].append(ChatMessage(role=role, content=content))

@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
def embed_file(file):
    path = f".cache/files/{file.name}"
    with open(path, "wb") as f:
        f.write(file.read())

    docs = PDFPlumberLoader(path).load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    splits = splitter.split_documents(docs)

    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(splits, embeddings)
    return vectorstore.as_retriever()

def create_chain(retriever, model_name="gpt-4o"):
    # 1) YAMLì—ì„œ prompt ìŠ¤í™ ë¡œë“œ
    with open("prompts/pdf-rag.yaml", encoding="utf-8") as f:
        prompt_spec = yaml.safe_load(f)

    # 2) PromptTemplate ê°ì²´ ìƒì„±
    prompt_template = PromptTemplate(
        template=prompt_spec["template"],
        input_variables=prompt_spec["input_variables"],
    )

    # 3) LLM ê°ì²´
    llm = ChatOpenAI(model_name=model_name, temperature=0)

    # 4) Runnable íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    #    retrieverì—ì„œ context, RunnablePassthroughë¡œ question ë°›ì•„
    #    â†’ PromptTemplate (Runnable) â†’ LLM â†’ StrOutputParser
    return (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt_template
        | llm
        | StrOutputParser()
    )

# íŒŒì¼ ì—…ë¡œë“œ ì‹œ
if uploaded_file:
    retriever = embed_file(uploaded_file)
    st.session_state["chain"] = create_chain(retriever, selected_model)

if clear_btn:
    st.session_state["messages"].clear()

print_messages()

user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
warning = st.empty()

if user_input:
    chain = st.session_state["chain"]
    if chain:
        st.chat_message("user").write(user_input)
        response = chain.stream(user_input)
        with st.chat_message("assistant"):
            container = st.empty()
            answer = ""
            for token in response:
                answer += token
                container.markdown(answer)
        add_message("user", user_input)
        add_message("assistant", answer)
    else:
        warning.error("íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")
