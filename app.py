# app.py
import os
import yaml
import streamlit as st
from dotenv import load_dotenv

from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain.callbacks.tracers import LangChainTracer

from rag.loader import load_and_split
from rag.embedder import get_embeddings, create_vectorstore
from rag.retriever import HybridRetriever
from rag.translator import build_translator_chain

# 1) í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2) LangChain Tracer ìƒì„±
tracer = LangChainTracer()

# ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„
os.makedirs(".cache/files", exist_ok=True)
os.makedirs(".cache/embeddings", exist_ok=True)

st.title("PDF ê¸°ë°˜ QAğŸ’¬")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "rag_chain" not in st.session_state:
    st.session_state["rag_chain"] = None
# ğŸ”¥ ë³€ê²½: ì—…ë¡œë“œëœ íŒŒì¼ íŠ¸ë˜í‚¹ìš©
if "uploaded_ids" not in st.session_state:
    st.session_state["uploaded_ids"] = None

# â€” ì‚¬ì´ë“œë°” UI â€”
with st.sidebar:
    clear_btn      = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    uploaded_files = st.file_uploader(
        "PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"], accept_multiple_files=True
    )
    selected_model = st.selectbox(
        "LLM ì„ íƒ", ["gpt-4o", "gpt-4-turbo", "gpt-4o-mini"], index=0
    )
    temperature     = st.slider(
        "ì˜¨ë„ ì„¤ì • (temperature)", 0.0, 1.0, 0.0, 0.01
    )
    translator_chain = build_translator_chain(
        model_name=selected_model,
        temperature=temperature
    )

def print_messages():
    for m in st.session_state["messages"]:
        st.chat_message(m.role).write(m.content)

def add_message(role, content):
    st.session_state["messages"].append(ChatMessage(role=role, content=content))

def create_chain(retriever_fn, model_name="gpt-4o", temperature=0.0):
    with open("prompts/pdf-rag.yaml", encoding="utf-8") as f:
        spec = yaml.safe_load(f)
    prompt = PromptTemplate(
        template=spec["template"],
        input_variables=spec["input_variables"],
    )
    llm = ChatOpenAI(
        model_name=model_name,
        temperature=temperature,
        callbacks=[tracer]
    )
    return (
        {"context": retriever_fn, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

# â€” ë©”ì¸ ë¡œì§ â€” 
# ğŸ”¥ ë³€ê²½: ì´ˆê¸°í™” ë¸”ë¡ â€” íŒŒì¼ ì—…ë¡œë“œê°€ ìƒˆë¡œ ê°ì§€ë  ë•Œë§Œ heavy ì‘ì—… ì‹¤í–‰
if uploaded_files:
    current_ids = tuple(f.name for f in uploaded_files)
    if st.session_state["uploaded_ids"] != current_ids:
        # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ ê°ì§€ â†’ ì²˜ë¦¬ ì‹œì‘
        st.session_state["uploaded_ids"] = current_ids
        # ë¬¸ì„œë³„ í”„ë¡œê·¸ë ˆìŠ¤ë°”
        total = len(uploaded_files)
        progress = st.progress(0)
        split_docs = []
        title_ph = st.empty()
        for idx, f in enumerate(uploaded_files):
            title_ph.info(f"ğŸ”„ ì²˜ë¦¬ ì¤‘: {f.name}")
            docs = load_and_split([f])  # cached
            split_docs.extend(docs)
            progress.progress(int((idx+1)/total * 100))
        title_ph.empty()
        progress.empty()

        # ì„ë² ë”©
        with st.spinner("ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ ì¤‘..."):
            embeddings = get_embeddings()
        # ë²¡í„°ìŠ¤í† ì–´
        with st.spinner("FAISS ìŠ¤í† ì–´ ìƒì„± ì¤‘..."):
            vectorstore = create_vectorstore(split_docs, embeddings)
        # ë¦¬íŠ¸ë¦¬ë²„Â·ì²´ì¸ ìƒì„±
        with st.spinner("RAG ì²´ì¸ ì´ˆê¸°í™” ì¤‘..."):
            vec_ret = vectorstore.as_retriever()
            hybrid = HybridRetriever(vec_ret, split_docs)
            st.session_state["rag_chain"] = create_chain(
                hybrid.get_relevant_documents,
                model_name=selected_model,
                temperature=temperature
            )

if clear_btn:
    st.session_state["messages"].clear()
    # ğŸ”¥ ë³€ê²½: ì—…ë¡œë“œ ë¦¬ì…‹
    st.session_state["uploaded_ids"] = None
    st.session_state["rag_chain"]    = None

print_messages()

user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
warning = st.empty()

if user_input:
    if st.session_state["rag_chain"] is None:
        warning.error("ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•˜ì„¸ìš”.")
    else:
        # í•œê¸€â†’ì˜ì–´ ë²ˆì—­
        eng_q = translator_chain.predict(text=user_input)
        st.chat_message("assistant").write(f"ğŸ”„ ë²ˆì—­ ì§ˆë¬¸: {eng_q}")

        # RAG í˜¸ì¶œ
        st.chat_message("user").write(user_input)
        response = st.session_state["rag_chain"].stream(eng_q)
        with st.chat_message("assistant"):
            container, ans = st.empty(), ""
            for t in response:
                ans += t
                container.markdown(ans)
        add_message("user", user_input)
        add_message("assistant", ans)
