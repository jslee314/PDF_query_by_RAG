# app.py
import os
import yaml
import streamlit as st
from dotenv import load_dotenv

from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain.callbacks.tracers import LangChainTracer

from rag.loader import load_and_split
from rag.embedder import get_embeddings, create_vectorstore
from rag.retriever import HybridRetriever
from rag.translator import build_translator_chain

# 1) í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2) LangChain Tracer
tracer = LangChainTracer()

# ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„
os.makedirs(".cache/files", exist_ok=True)
os.makedirs(".cache/embeddings", exist_ok=True)

st.title("PDF ê¸°ë°˜ QAğŸ’¬")

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
# ğŸ”¥ ë³€ê²½: íŒŒì¼ë³„ ë¶„í• /ë¦¬íŠ¸ë¦¬ë²„/ì²´ì¸ ì €ì¥ìš©
if "split_docs_by_file" not in st.session_state:
    st.session_state["split_docs_by_file"] = {}
if "file_retrievers" not in st.session_state:
    st.session_state["file_retrievers"] = {}
if "file_chains" not in st.session_state:
    st.session_state["file_chains"] = {}

with st.sidebar:
    clear_btn      = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    uploaded_files = st.file_uploader(
        "PDF íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf"],
        accept_multiple_files=True
    )
    selected_model = st.selectbox(
        "LLM ì„ íƒ",
        ["gpt-4o", "gpt-4-turbo", "gpt-4o-mini"],
        index=0
    )
    temperature     = st.slider(
        "ì˜¨ë„ ì„¤ì • (temperature)",
        0.0, 1.0, 0.0, 0.01
    )
    translator_chain = build_translator_chain(
        model_name=selected_model,
        temperature=temperature
    )

def print_messages():
    for msg in st.session_state["messages"]:
        st.chat_message(msg.role).write(msg.content)

def add_message(role, content):
    st.session_state["messages"].append(ChatMessage(role=role, content=content))

def create_chain(retriever_fn, model_name="gpt-4o", temperature=0.0):
    with open("prompts/pdf-rag.yaml", encoding="utf-8") as f:
        spec = yaml.safe_load(f)
    prompt = PromptTemplate(
        template=spec["template"],
        input_variables=spec["input_variables"],
    )
    llm = ChatOpenAI(
        model_name=model_name,
        temperature=temperature,
        callbacks=[tracer]
    )
    return (
        {"context": retriever_fn, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

# â€” ë¬¸ì„œ ì²˜ë¦¬: ì—…ë¡œë“œëœ íŒŒì¼ë³„ë¡œ ë¶„í• , ì„ë² ë”©, ë¦¬íŠ¸ë¦¬ë²„, ì²´ì¸ ìƒì„± â€” #
if uploaded_files:
    total = len(uploaded_files)
    progress = st.progress(0)
    split_docs_by_file = {}
    file_retrievers = {}
    file_chains = {}

    for idx, f in enumerate(uploaded_files):
        st.sidebar.info(f"â³ ì²˜ë¦¬ ì¤‘: **{f.name}**")
        # 1) íŒŒì¼ë³„ ë¶„í• 
        docs = load_and_split([f])  # ğŸ”¥ ìºì‹œë¨
        split_docs_by_file[f.name] = docs

        # 2) íŒŒì¼ë³„ ë²¡í„°ìŠ¤í† ì–´ & ë¦¬íŠ¸ë¦¬ë²„
        vs = create_vectorstore(docs, get_embeddings())
        file_retrievers[f.name] = vs.as_retriever()

        # 3) íŒŒì¼ë³„ RAG ì²´ì¸
        file_chains[f.name] = create_chain(
            file_retrievers[f.name],
            model_name=selected_model,
            temperature=temperature
        )
        progress.progress(int((idx + 1) / total * 100))

    progress.empty()
    st.session_state["split_docs_by_file"] = split_docs_by_file
    st.session_state["file_retrievers"]    = file_retrievers
    st.session_state["file_chains"]        = file_chains

if clear_btn:
    st.session_state["messages"].clear()
    st.session_state["split_docs_by_file"] = {}
    st.session_state["file_retrievers"]    = {}
    st.session_state["file_chains"]        = {}

print_messages()

# â€” ì§ˆë¬¸ ì²˜ë¦¬ â€” #
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
warning = st.empty()

if user_input:
    if not st.session_state["file_chains"]:
        warning.error("ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•˜ì„¸ìš”.")
    else:
        # 1) í•œê¸€â†’ì˜ì–´ ë²ˆì—­
        eng_q = translator_chain.predict(text=user_input)
        st.chat_message("assistant").write(f"ğŸ”„ ë²ˆì—­ëœ ì§ˆë¬¸: {eng_q}")

        # 2) íŒŒì¼ë³„ ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°
        for fname, chain in st.session_state["file_chains"].items():
            st.markdown(f"### ğŸ“„ {fname}")
            with st.chat_message("assistant"):
                container = st.empty()
                answer = ""
                for token in chain.stream(eng_q):  # ğŸ”¥ stream() ì‚¬ìš©
                    answer += token
                    container.markdown(answer)
            st.write("---")

        # 3) ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        add_message("user", user_input)
        # ì „ì²´ íŒŒì¼ë³„ ë‹µë³€ì„ í•˜ë‚˜ë¡œ í•©ì³ ê¸°ë¡
        combined = "\n\n".join(
            f"## {fname}\n{chain.invoke(eng_q)}"
            for fname, chain in st.session_state["file_chains"].items()
        )
        add_message("assistant", combined)
