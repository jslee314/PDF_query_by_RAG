import os
import yaml
import streamlit as st
from dotenv import load_dotenv

from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain.callbacks.tracers import LangChainTracer

from rag.loader import load_and_split
from rag.embedder import get_embeddings, create_vectorstore
from rag.retriever import HybridRetriever
from rag.translator import build_translator_chain

import index_processor
import qa_processor

# 1) í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2) LangChain Tracer ìƒì„±
tracer = LangChainTracer()

# ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„
os.makedirs(".cache/files", exist_ok=True)
os.makedirs(".cache/embeddings", exist_ok=True)

st.title("PDF ê¸°ë°˜ QAğŸ’¬")

# ì‚¬ì´ë“œë°”: ëª¨ë“œ ì„ íƒ
mode = st.sidebar.radio("ëª¨ë“œ ì„ íƒ", ["ë¬¸ì„œ ì²˜ë¦¬", "ì§ˆë¬¸ ì‘ë‹µ"])

# ê³µí†µ ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ì‚¬ì´ë“œë°”: íŒŒì¼ ì—…ë¡œë“œ, ëª¨ë¸, ì˜¨ë„, ë¡œë”/ìŠ¤í”Œë¦¿í„° ì˜µì…˜ ì„ íƒ
uploaded_files = st.sidebar.file_uploader(
    "PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"], accept_multiple_files=True
)
selected_model = st.sidebar.selectbox(
    "LLM ì„ íƒ", ["gpt-4o", "gpt-4-turbo", "gpt-4o-mini"], index=0
)
temperature = st.sidebar.slider(
    "ì˜¨ë„ ì„¤ì • (temperature)", 0.0, 1.0, 0.0, 0.01
)
loader_option = st.sidebar.selectbox(
    "ë¬¸ì„œ ë¡œë” ì„ íƒ",
    ["PyPDFLoader", "PDFPlumberLoader", "UnstructuredPDFLoader"],
    index=1
)
splitter_option = st.sidebar.selectbox(
    "ìŠ¤í”Œë¦¿í„° ì„ íƒ",
    ["RecursiveCharacterTextSplitter", "SpacyTextSplitter", "TokenTextSplitter"],
    index=0
)

# ì‚¬ì´ë“œë°”: YAML í”„ë¡¬í”„íŠ¸ ë¡œë“œ & í¸ì§‘
with st.sidebar.expander("ğŸ”§ RAG í”„ë¡¬í”„íŠ¸ í¸ì§‘"):
    raw_spec = yaml.safe_load(open("prompts/pdf-rag.yaml", encoding="utf-8"))
    default_template = raw_spec["template"]
    input_vars       = raw_spec["input_variables"]
    edited_template  = st.text_area(
        "Prompt Template", value=default_template, height=200
    )

translator_chain = build_translator_chain(
    model_name=selected_model,
    temperature=temperature
)

# ëª¨ë“œë³„ ë Œë”ë§
if mode == "ë¬¸ì„œ ì²˜ë¦¬":
    index_processor.render(
        uploaded_files=uploaded_files,
        selected_model=selected_model,
        temperature=temperature,
        translator_chain=translator_chain,
        tracer=tracer,
        loader_option=loader_option,
        splitter_option=splitter_option,
        prompt_template=edited_template,
        input_variables=input_vars,
    )
elif mode == "ì§ˆë¬¸ ì‘ë‹µ":
    qa_processor.render(
        user_input=st.session_state.get("last_question", ""),
        translator_chain=translator_chain,
        tracer=tracer
    )
